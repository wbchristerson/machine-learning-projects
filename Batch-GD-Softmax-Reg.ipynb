{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gothic-clothing",
   "metadata": {},
   "source": [
    "### An Implementation Of Batch Gradient Descent With Early Stopping For Softmax Regression Without Using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eight-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2042)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_test_split(X, y, test_ratio = 0.2, validation_ratio = 0.2):\n",
    "    total_size = len(X)\n",
    "    test_size = int(total_size * test_ratio)\n",
    "    valid_size = int(total_size * validation_ratio)\n",
    "    train_size = total_size - test_size - valid_size\n",
    "    \n",
    "    permutation_indices = np.random.permutation(total_size)\n",
    "    \n",
    "    X_train = X[permutation_indices[:train_size]]\n",
    "    y_train = y[permutation_indices[:train_size]]\n",
    "    X_valid = X[permutation_indices[train_size:-test_size]]\n",
    "    y_valid = y[permutation_indices[train_size:-test_size]]\n",
    "    X_test = X[permutation_indices[-test_size:]]\n",
    "    y_test = y[permutation_indices[-test_size:]]\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(y_output, possible_outputs):\n",
    "    m = len(y_output)\n",
    "    encoding = np.zeros((m, possible_outputs))\n",
    "    encoding[np.arange(m), y_output] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuing-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2,3)]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = my_test_split(X_with_bias, y)\n",
    "\n",
    "y_train_one_hot = encode_output(y_train, len(np.unique(y)))\n",
    "y_valid_one_hot = encode_output(y_valid, len(np.unique(y)))\n",
    "y_test_one_hot = encode_output(y_test, len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exps_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exps_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-january",
   "metadata": {},
   "source": [
    "$J(\\mathbf{\\Theta}) =\n",
    "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "\n",
    "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "corresponding-carrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 1.6256927652424578\n",
      "[[0.64169972 0.4312359  0.37721392]\n",
      " [0.41944362 0.31001648 0.9673021 ]\n",
      " [0.87074017 0.41463444 0.96446695]]\n",
      "[[0.17593295 0.09611022 0.93567336]\n",
      " [0.75818844 0.17192172 0.93019649]]\n",
      "3.0680231831519\n",
      "\n",
      "iteration: 500, loss: 0.7501153624129366\n",
      "[[ 1.35293294  0.29544561 -0.19822901]\n",
      " [ 0.24106854  0.74861486  0.7070788 ]\n",
      " [ 0.55618596  0.54389953  1.14975608]]\n",
      "[[0.05811404 0.56042421 0.49996043]\n",
      " [0.30934282 0.2958267  1.32193903]]\n",
      "3.045607232779993\n",
      "\n",
      "iteration: 1000, loss: 0.633863423393384\n",
      "[[ 1.89681574  0.14017898 -0.58684518]\n",
      " [ 0.15746895  0.80807453  0.73121873]\n",
      " [ 0.34295402  0.53323638  1.37365116]]\n",
      "[[0.02479647 0.65298445 0.53468083]\n",
      " [0.11761746 0.28434103 1.88691751]]\n",
      "3.5013377467419904\n",
      "\n",
      "iteration: 1500, loss: 0.5631385364010538\n",
      "[[ 2.31316518  0.03738318 -0.90039882]\n",
      " [ 0.08619155  0.85943002  0.75114063]\n",
      " [ 0.1801523   0.50959716  1.5600921 ]]\n",
      "[[0.00742898 0.73861996 0.56421224]\n",
      " [0.03245485 0.25968927 2.43388736]]\n",
      "4.036292668045343\n",
      "\n",
      "iteration: 2000, loss: 0.515745909839242\n",
      "[[ 2.64661483 -0.02938788 -1.16707741]\n",
      " [ 0.02575338  0.90321867  0.76779015]\n",
      " [ 0.0504053   0.47837857  1.72105769]]\n",
      "[[6.63236561e-04 8.15803963e-01 5.89501721e-01]\n",
      " [2.54069387e-03 2.28846060e-01 2.96203957e+00]]\n",
      "4.599395247171856\n",
      "\n",
      "iteration: 2500, loss: 0.48134571795893716\n",
      "[[ 2.92432378 -0.07163778 -1.40253645]\n",
      " [-0.02607021  0.94068674  0.78214568]\n",
      " [-0.057044    0.44339341  1.86349215]]\n",
      "[[6.79656067e-04 8.84891537e-01 6.11751863e-01]\n",
      " [3.25401776e-03 1.96597719e-01 3.47260298e+00]]\n",
      "5.169777770479214\n",
      "\n",
      "iteration: 3000, loss: 0.4548259873213263\n",
      "[[ 3.16283465 -0.09681734 -1.61586777]\n",
      " [-0.07116459  0.97297346  0.79495332]\n",
      " [-0.14880507  0.40699054  1.9916561 ]]\n",
      "[[0.0050644  0.94667736 0.63195079]\n",
      " [0.02214295 0.1656413  3.96669401]]\n",
      "5.738170801157866\n",
      "\n",
      "iteration: 3500, loss: 0.4334474436606628\n",
      "[[ 3.37261455 -0.10978819 -1.81267682]\n",
      " [-0.11096415  1.00098986  0.80673649]\n",
      " [-0.2290784   0.37056915  2.10835082]]\n",
      "[[0.01231304 1.00198069 0.65082377]\n",
      " [0.05247692 0.13732149 4.44514317]]\n",
      "6.300059081641629\n",
      "\n",
      "iteration: 4000, loss: 0.4156277879657445\n",
      "[[ 3.56057628 -0.11380969 -1.99661705]\n",
      " [-0.14653845  1.02544719  0.81785346]\n",
      " [-0.30064399  0.33495001  2.21553554]]\n",
      "[[0.02147352 1.05154194 0.66888428]\n",
      " [0.09038681 0.11219151 4.90859773]]\n",
      "6.853075777459194\n",
      "\n",
      "iteration: 4500, loss: 0.4003922755401664\n",
      "[[ 3.73146833 -0.11113322 -2.17018557]\n",
      " [-0.17868927  1.04690361  0.82854786]\n",
      " [-0.36541296  0.30060282  2.3146517 ]]\n",
      "[[0.03192986 1.09600718 0.68649155]\n",
      " [0.13352663 0.09036206 5.35761248]]\n",
      "7.395929755998501\n",
      "\n",
      "iteration: 5000, loss: 0.3871071436337204\n",
      "[[ 3.88866604 -0.10335776 -2.33515874]\n",
      " [-0.20802571  1.0658029   0.83898501]\n",
      " [-0.42474378  0.26778211  2.40680323]]\n",
      "[[0.04327469 1.13593582 0.70389584]\n",
      " [0.18040728 0.07170726 5.79270177]]\n",
      "7.927922658196436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y))\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "iterations = 5001\n",
    "eta = 0.01\n",
    "\n",
    "theta = np.random.rand(n_inputs, n_outputs)\n",
    "for i in range(iterations):\n",
    "    logits = X_train.dot(theta)\n",
    "    y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(np.log(y_proba + epsilon) * y_train_one_hot, axis = 1))\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration: {i}, loss: {loss}\")\n",
    "        print(theta)\n",
    "        print(np.square(theta[1:]))\n",
    "        print(np.sum(np.square(theta[1:])))\n",
    "        print()\n",
    "    \n",
    "    error = y_proba - y_train_one_hot\n",
    "#     print(y_proba[0])\n",
    "#     print(y_train_one_hot[0])\n",
    "#     print(error[0])\n",
    "    gradients = 1/m * (X_train.T.dot(error))\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "successful-cannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.51152678, -0.24775141, -2.26775028],\n",
       "       [-0.36420824,  0.64468723,  0.36937679],\n",
       "       [-1.0708845 ,  0.2609794 ,  2.41911317]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "monthly-elephant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-mumbai",
   "metadata": {},
   "source": [
    "### With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "desirable-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 1.301694722520997\n",
      "iteration: 500, loss: 0.3928030617844213\n",
      "iteration: 1000, loss: 0.30658759857848306\n",
      "iteration: 1500, loss: 0.2621561380281134\n",
      "iteration: 2000, loss: 0.23316928328420883\n",
      "iteration: 2500, loss: 0.21228612806606428\n",
      "iteration: 3000, loss: 0.1963633650154607\n",
      "iteration: 3500, loss: 0.18375171897389989\n",
      "iteration: 4000, loss: 0.17347908052506197\n",
      "iteration: 4500, loss: 0.16492763625647988\n",
      "iteration: 5000, loss: 0.15768316862309792\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y))\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "iterations = 5001\n",
    "eta = 0.1\n",
    "\n",
    "r_theta = np.random.rand(n_inputs, n_outputs)\n",
    "for i in range(iterations):\n",
    "    logits = X_train.dot(r_theta)\n",
    "    y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(np.log(y_proba + epsilon) * y_train_one_hot, axis = 1))\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration: {i}, loss: {loss}\")\n",
    "    \n",
    "    error = y_proba - y_train_one_hot\n",
    "    gradients = 1/m * (X_train.T.dot(error))\n",
    "    r_theta = r_theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bigger-governor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(r_theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "color-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient_scalar(x_i, theta_matrix, output_class_k, true_outcome):\n",
    "#     '''return p_k^(i) hat - y_k^(i)'''\n",
    "    \n",
    "#     matrix_product = x_i @ theta_matrix\n",
    "#     pki_hat = np.exp(matrix_product[output_class_k]) / np.sum(np.exp(matrix_product)) # probability\n",
    "    \n",
    "#     yki = 1 if output_class_k == true_outcome else 0\n",
    "#     return pki_hat - yki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "professional-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_theta_gradient(output_class_k, X, y, theta_matrix):\n",
    "#     theta_gradient = np.zeros((1, X.shape[1]))\n",
    "#     m = X.shape[0]\n",
    "#     for x_i, true_outcome in zip(X, y):\n",
    "#         theta_gradient += get_gradient_scalar(x_i, theta_matrix, output_class_k, true_outcome) * x_i\n",
    "    \n",
    "#     return theta_gradient / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indie-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0, t1 = 1, 1\n",
    "\n",
    "# def get_step_multiplier(alpha, epoch):\n",
    "#     return alpha * (t0 / (t1 + epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mathematical-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def my_batch_GD(X_train, y_train, X_valid, y_valid, num_output_classes, alpha = 1, epochs = 10):\n",
    "#     theta_matrix = np.random.rand(X_train[0].size, num_output_classes)\n",
    "#     best_theta_matrix = theta_matrix\n",
    "#     best_accuracy_score = accuracy_score(y_valid, np.argmax(X_valid @ theta_matrix, axis=1))\n",
    "    \n",
    "#     for e in range(epochs):\n",
    "#         delta_theta_matrix = np.array([]).reshape((0, X_train.shape[1]))\n",
    "# #         print(f\"delta_theta_matrix: {delta_theta_matrix}\")\n",
    "#         for k in range(num_output_classes):\n",
    "#             delta_theta_matrix = np.vstack(\n",
    "#                 (delta_theta_matrix, np.array(get_theta_gradient(k, X_train, y_train, theta_matrix)))\n",
    "#             )\n",
    "# #         print(f\"delta_theta_matrix: \\n{delta_theta_matrix}, \\ntheta_matrix: \\n{theta_matrix}\")\n",
    "        \n",
    "#         step_multiplier = get_step_multiplier(alpha, e)\n",
    "#         theta_matrix -= step_multiplier * np.transpose(delta_theta_matrix)\n",
    "        \n",
    "#         temp_accuracy = accuracy_score(y_valid, np.argmax(X_valid @ theta_matrix, axis=1))\n",
    "#         if temp_accuracy > best_accuracy_score:\n",
    "#             best_accuracy_score = temp_accuracy\n",
    "#             best_theta_matrix = np.copy(theta_matrix)\n",
    "    \n",
    "#     print(\"theta_matrix:\")\n",
    "#     print(theta_matrix)\n",
    "#     print(\"\\nbest_theta_matrix:\")\n",
    "#     print(best_theta_matrix)\n",
    "#     return best_theta_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "grand-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_matrix:\n",
      "[[ 2.05066161  1.34734224 -2.02030864]\n",
      " [ 1.94817848  2.20419274 -1.91676995]\n",
      " [ 1.7032019   2.36560163 -2.27063944]]\n",
      "\n",
      "best_theta_matrix:\n",
      "[[ 2.05066161  1.34734224 -2.02030864]\n",
      " [ 1.94817848  2.20419274 -1.91676995]\n",
      " [ 1.7032019   2.36560163 -2.27063944]]\n",
      "[[ 2.05066161  1.34734224 -2.02030864]\n",
      " [ 1.94817848  2.20419274 -1.91676995]\n",
      " [ 1.7032019   2.36560163 -2.27063944]]\n"
     ]
    }
   ],
   "source": [
    "# print(my_batch_GD(np.array( [ [1,2,3], [4,5,6], [7,8,9], [10,11,12] ] ), np.array([0, 1, 1, 0]),\n",
    "#                   np.array( [ [13,14,15] ] ), np.array([1]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hollywood-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_and_make_prediction(X_train, y_train, X_valid, y_valid, num_output_classes, alpha=1.0, epochs = 10):\n",
    "#     theta_matrix = my_batch_GD(X_train, y_train, X_valid, y_valid, num_output_classes, alpha, epochs)\n",
    "    \n",
    "# #     print(\"theta_matrix:\")\n",
    "# #     print(theta_matrix)\n",
    "    \n",
    "#     valid_relative_weights = X_valid @ theta_matrix\n",
    "#     train_relative_weights = X_train @ theta_matrix\n",
    "#     return np.argmax(valid_relative_weights, axis=1), np.argmax(train_relative_weights, axis=1), theta_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "needed-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reference regressor\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=11)\n",
    "# softmax_reg.fit(X_with_bias, y)\n",
    "\n",
    "# y_pred = softmax_reg.predict(X_with_bias)\n",
    "# matches = len(y_pred[y_pred == y])\n",
    "# total = len(y_pred)\n",
    "# print(len(y_pred[y_pred == y]))\n",
    "# print(len(y_pred))\n",
    "# print(f\"Logistic Regression: {matches} / {total}: {matches / total}\")\n",
    "# print(f\"Accuracy Score: {accuracy_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "virgin-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_pred, train_pred, my_theta_matrix = fit_and_make_prediction(X_train, y_train, X_valid, y_valid, 3, epochs = 50)\n",
    "\n",
    "# valid_pred_count = len(valid_pred[valid_pred == y_valid])\n",
    "# train_pred_count = len(train_pred[train_pred == y_train])\n",
    "\n",
    "# print(f\"\\nmy validation count: {valid_pred_count} / {len(valid_pred)}\")\n",
    "# print(f\"validation accuracy score: {accuracy_score(y_valid, valid_pred)}\")\n",
    "\n",
    "# print(f\"my training count: {train_pred_count} / {len(train_pred)}\")\n",
    "# print(f\"training accuracy score: {accuracy_score(y_train, train_pred)}\")\n",
    "\n",
    "# print(f\"\\nmy validation predictions  : {valid_pred}\")\n",
    "# print(f\"true validation predictions: {y_valid}\")\n",
    "\n",
    "# print()\n",
    "# print(f\"my training predictions  : {train_pred}\")\n",
    "# print(f\"true training predictions: {y_train}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
