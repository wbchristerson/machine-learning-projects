{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gothic-clothing",
   "metadata": {},
   "source": [
    "### An Implementation Of Batch Gradient Descent With Early Stopping For Softmax Regression Without Using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eight-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_test_split(X, y, test_ratio = 0.2, validation_ratio = 0.2):\n",
    "    total_size = len(X)\n",
    "    test_size = total_size * test_ratio\n",
    "    valid_size = total_size * validation_ratio\n",
    "    train_size = total_size - test_size - valid_size\n",
    "    \n",
    "    permutation_indices = np.ranndom.permutation(total_size)\n",
    "    \n",
    "    X_train = X[permutation_indices[:train_size]]\n",
    "    y_train = y[permutation_indices[:train_size]]\n",
    "    X_valid = X[permutation_indices[train_size:-test_size]]\n",
    "    y_valid = y[permutation_indices[train_size:-test_size]]\n",
    "    X_test = X[permutation_indices[-test_size:]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "color-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_scalar(x_i, theta_matrix, output_class_k, true_outcome):\n",
    "    '''return p_k^(i) hat - y_k^(i)'''\n",
    "    \n",
    "    matrix_product = x_i @ theta_matrix\n",
    "    pki_hat = np.exp(matrix_product[output_class_k]) / np.sum(np.exp(matrix_product)) # probability\n",
    "    \n",
    "    yki = 1 if output_class_k == true_outcome else 0\n",
    "    return pki_hat - yki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "professional-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta_gradient(output_class_k, X, y, theta_matrix):\n",
    "    theta_gradient = np.zeros((1, X.shape[1]))\n",
    "    m = X.shape[0]\n",
    "    for x_i, true_outcome in zip(X, y):\n",
    "        theta_gradient += get_gradient_scalar(x_i, theta_matrix, output_class_k, true_outcome) * x_i\n",
    "    \n",
    "    return theta_gradient / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "indie-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = 1, 1\n",
    "\n",
    "def get_step_multiplier(alpha, epoch):\n",
    "    return alpha * (t0 / (t1 + epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "mathematical-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_batch_GD(X, y, num_output_classes, alpha = 1, epochs = 10):\n",
    "    theta_matrix = np.random.rand(X[0].size, num_output_classes)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        delta_theta_matrix = np.array([]).reshape((0, X.shape[1]))\n",
    "#         print(f\"delta_theta_matrix: {delta_theta_matrix}\")\n",
    "        for k in range(num_output_classes):\n",
    "            delta_theta_matrix = np.vstack((delta_theta_matrix, np.array(get_theta_gradient(k, X, y, theta_matrix))))\n",
    "#         print(f\"delta_theta_matrix: \\n{delta_theta_matrix}, \\ntheta_matrix: \\n{theta_matrix}\")\n",
    "        \n",
    "        step_multiplier = get_step_multiplier(alpha, e)\n",
    "        theta_matrix -= step_multiplier * np.transpose(delta_theta_matrix)\n",
    "    return theta_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "grand-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.14574938  1.90136613 -3.19411938]\n",
      " [ 2.41716302  2.12054434 -4.1087619 ]\n",
      " [ 3.13768481  2.49425445 -4.62433411]]\n"
     ]
    }
   ],
   "source": [
    "print(my_batch_GD(np.array( [ [1,2,3], [4,5,6], [7,8,9], [10,11,12] ] ), np.array([0, 1, 1, 0]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "hollywood-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_make_prediction(X_train, y_train, X_test, num_output_classes, alpha=1.0, epochs = 10):\n",
    "    theta_matrix = my_batch_GD(X_train, y_train, num_output_classes, alpha, epochs)\n",
    "    print(theta_matrix)\n",
    "    relative_weights = X_test @ theta_matrix\n",
    "    return np.argmax(relative_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "different-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "equipped-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2,3)]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "needed-slovenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "150\n",
      "Logistic Regression: 144 / 150: 0.96\n",
      "Accuracy Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Reference regressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=11)\n",
    "softmax_reg.fit(X_with_bias, y)\n",
    "\n",
    "y_pred = softmax_reg.predict(X_with_bias)\n",
    "matches = len(y_pred[y_pred == y])\n",
    "total = len(y_pred)\n",
    "print(len(y_pred[y_pred == y]))\n",
    "print(len(y_pred))\n",
    "print(f\"Logistic Regression: {matches} / {total}: {matches / total}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "virgin-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57588492 0.72834736 0.63385184]\n",
      " [0.05161284 0.37855156 0.83367592]]\n",
      "My Implementation Of Batch Gradient Descent: 50 / 150\n",
      "Accuracy Score: 0.3333333333333333\n",
      "my predictions: [1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 2 1 1 1 2 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 2 2 1 2 2 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "my_imp_y_pred = fit_and_make_prediction(X, y, X, 3, epochs = 40)\n",
    "\n",
    "# print(my_imp_y_pred)\n",
    "my_imp_matches = len(my_imp_y_pred[my_imp_y_pred == y])\n",
    "my_imp_total = len(my_imp_y_pred)\n",
    "print(f\"My Implementation Of Batch Gradient Descent: {my_imp_matches} / {my_imp_total}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y, my_imp_y_pred)}\")\n",
    "print(f\"my predictions: {my_imp_y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-spoke",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
