{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gothic-clothing",
   "metadata": {},
   "source": [
    "### An Implementation Of Batch Gradient Descent With Early Stopping For Softmax Regression Without Using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eight-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2042)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "expensive-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_test_split(X, y, test_ratio = 0.2, validation_ratio = 0.2):\n",
    "    total_size = len(X)\n",
    "    test_size = int(total_size * test_ratio)\n",
    "    valid_size = int(total_size * validation_ratio)\n",
    "    train_size = total_size - test_size - valid_size\n",
    "    \n",
    "    permutation_indices = np.random.permutation(total_size)\n",
    "    \n",
    "    X_train = X[permutation_indices[:train_size]]\n",
    "    y_train = y[permutation_indices[:train_size]]\n",
    "    X_valid = X[permutation_indices[train_size:-test_size]]\n",
    "    y_valid = y[permutation_indices[train_size:-test_size]]\n",
    "    X_test = X[permutation_indices[-test_size:]]\n",
    "    y_test = X[permutation_indices[-test_size:]]\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "delayed-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(y_output, possible_outputs):\n",
    "    m = len(y_output)\n",
    "    encoding = np.zeros((m, possible_outputs))\n",
    "    encoding[np.arange(m), y_output] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "continuing-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2,3)]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = my_test_split(X_with_bias, encode_output(y, len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "distributed-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exps_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exps_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-january",
   "metadata": {},
   "source": [
    "$J(\\mathbf{\\Theta}) =\n",
    "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "\n",
    "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "corresponding-carrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3398890678549158\n",
      "\n",
      "[[-0.50491504 -1.52010368 -1.72735338]\n",
      " [-0.71980296 -1.24963939 -1.48481612]\n",
      " [-0.52870168 -1.45894568 -1.72513284]\n",
      " [-0.45839366 -1.55355148 -1.85657792]\n",
      " [-0.59994152 -1.36402782 -1.63205426]]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y))\n",
    "epsilon = 1e-7\n",
    "iterations = 1\n",
    "\n",
    "theta = np.random.rand(n_inputs, n_outputs)\n",
    "for i in range(iterations):\n",
    "    logits = X_train.dot(theta)\n",
    "    y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(np.log(y_proba + epsilon) * y_train, axis = 1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "color-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_scalar(x_i, theta_matrix, output_class_k, true_outcome):\n",
    "    '''return p_k^(i) hat - y_k^(i)'''\n",
    "    \n",
    "    matrix_product = x_i @ theta_matrix\n",
    "    pki_hat = np.exp(matrix_product[output_class_k]) / np.sum(np.exp(matrix_product)) # probability\n",
    "    \n",
    "    yki = 1 if output_class_k == true_outcome else 0\n",
    "    return pki_hat - yki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "professional-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta_gradient(output_class_k, X, y, theta_matrix):\n",
    "    theta_gradient = np.zeros((1, X.shape[1]))\n",
    "    m = X.shape[0]\n",
    "    for x_i, true_outcome in zip(X, y):\n",
    "        theta_gradient += get_gradient_scalar(x_i, theta_matrix, output_class_k, true_outcome) * x_i\n",
    "    \n",
    "    return theta_gradient / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indie-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = 1, 1\n",
    "\n",
    "def get_step_multiplier(alpha, epoch):\n",
    "    return alpha * (t0 / (t1 + epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mathematical-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def my_batch_GD(X_train, y_train, X_valid, y_valid, num_output_classes, alpha = 1, epochs = 10):\n",
    "    theta_matrix = np.random.rand(X_train[0].size, num_output_classes)\n",
    "    best_theta_matrix = theta_matrix\n",
    "    best_accuracy_score = accuracy_score(y_valid, np.argmax(X_valid @ theta_matrix, axis=1))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        delta_theta_matrix = np.array([]).reshape((0, X_train.shape[1]))\n",
    "#         print(f\"delta_theta_matrix: {delta_theta_matrix}\")\n",
    "        for k in range(num_output_classes):\n",
    "            delta_theta_matrix = np.vstack(\n",
    "                (delta_theta_matrix, np.array(get_theta_gradient(k, X_train, y_train, theta_matrix)))\n",
    "            )\n",
    "#         print(f\"delta_theta_matrix: \\n{delta_theta_matrix}, \\ntheta_matrix: \\n{theta_matrix}\")\n",
    "        \n",
    "        step_multiplier = get_step_multiplier(alpha, e)\n",
    "        theta_matrix -= step_multiplier * np.transpose(delta_theta_matrix)\n",
    "        \n",
    "        temp_accuracy = accuracy_score(y_valid, np.argmax(X_valid @ theta_matrix, axis=1))\n",
    "        if temp_accuracy > best_accuracy_score:\n",
    "            best_accuracy_score = temp_accuracy\n",
    "            best_theta_matrix = np.copy(theta_matrix)\n",
    "    \n",
    "    print(\"theta_matrix:\")\n",
    "    print(theta_matrix)\n",
    "    print(\"\\nbest_theta_matrix:\")\n",
    "    print(best_theta_matrix)\n",
    "    return best_theta_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "grand-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_matrix:\n",
      "[[0.76113877 0.69342476 0.48133725]\n",
      " [0.65674116 0.97044804 0.23801772]\n",
      " [1.02029329 0.42895845 0.46294118]]\n",
      "\n",
      "best_theta_matrix:\n",
      "[[-1.71797406  3.16133731  0.49253752]\n",
      " [-2.26535748  3.8710058   0.2595586 ]\n",
      " [-2.34479115  3.76216141  0.49482266]]\n",
      "[[-1.71797406  3.16133731  0.49253752]\n",
      " [-2.26535748  3.8710058   0.2595586 ]\n",
      " [-2.34479115  3.76216141  0.49482266]]\n"
     ]
    }
   ],
   "source": [
    "print(my_batch_GD(np.array( [ [1,2,3], [4,5,6], [7,8,9], [10,11,12] ] ), np.array([0, 1, 1, 0]),\n",
    "                  np.array( [ [13,14,15] ] ), np.array([1]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollywood-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_make_prediction(X_train, y_train, X_valid, y_valid, num_output_classes, alpha=1.0, epochs = 10):\n",
    "    theta_matrix = my_batch_GD(X_train, y_train, X_valid, y_valid, num_output_classes, alpha, epochs)\n",
    "    \n",
    "#     print(\"theta_matrix:\")\n",
    "#     print(theta_matrix)\n",
    "    \n",
    "    valid_relative_weights = X_valid @ theta_matrix\n",
    "    train_relative_weights = X_train @ theta_matrix\n",
    "    return np.argmax(valid_relative_weights, axis=1), np.argmax(train_relative_weights, axis=1), theta_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "needed-slovenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "150\n",
      "Logistic Regression: 144 / 150: 0.96\n",
      "Accuracy Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Reference regressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=11)\n",
    "softmax_reg.fit(X_with_bias, y)\n",
    "\n",
    "y_pred = softmax_reg.predict(X_with_bias)\n",
    "matches = len(y_pred[y_pred == y])\n",
    "total = len(y_pred)\n",
    "print(len(y_pred[y_pred == y]))\n",
    "print(len(y_pred))\n",
    "print(f\"Logistic Regression: {matches} / {total}: {matches / total}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "virgin-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_matrix:\n",
      "[[ 1.26164891  0.0095573  -0.33291927]\n",
      " [ 0.19846246  0.8023991   0.93816814]\n",
      " [ 0.34243093  0.40465132  0.35505729]]\n",
      "\n",
      "best_theta_matrix:\n",
      "[[ 1.02195519  0.04039591 -0.12406416]\n",
      " [ 0.2423502   0.79856735  0.89811216]\n",
      " [ 0.44094503  0.45976293  0.20143157]]\n",
      "\n",
      "my validation count: 23 / 30\n",
      "validation accuracy score: 0.7666666666666667\n",
      "my training count: 57 / 90\n",
      "training accuracy score: 0.6333333333333333\n",
      "\n",
      "my validation predictions  : [0 0 0 0 1 2 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1]\n",
      "true validation predictions: [0 0 0 0 2 2 2 2 0 1 0 1 1 2 1 0 2 1 0 1 2 0 0 0 0 0 1 0 0 1]\n",
      "\n",
      "my training predictions  : [0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
      " 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0]\n",
      "true training predictions: [0 0 1 1 2 2 2 1 0 2 0 1 1 1 0 1 1 1 1 0 2 0 2 1 2 2 0 2 2 1 1 1 2 2 1 1 1\n",
      " 2 2 2 1 2 0 0 1 2 2 2 1 1 0 0 2 1 1 1 0 2 2 1 0 0 0 2 1 2 2 1 0 0 2 1 0 1\n",
      " 2 0 0 2 2 0 0 0 2 1 2 1 2 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "valid_pred, train_pred, my_theta_matrix = fit_and_make_prediction(X_train, y_train, X_valid, y_valid, 3, epochs = 50)\n",
    "\n",
    "valid_pred_count = len(valid_pred[valid_pred == y_valid])\n",
    "train_pred_count = len(train_pred[train_pred == y_train])\n",
    "\n",
    "print(f\"\\nmy validation count: {valid_pred_count} / {len(valid_pred)}\")\n",
    "print(f\"validation accuracy score: {accuracy_score(y_valid, valid_pred)}\")\n",
    "\n",
    "print(f\"my training count: {train_pred_count} / {len(train_pred)}\")\n",
    "print(f\"training accuracy score: {accuracy_score(y_train, train_pred)}\")\n",
    "\n",
    "print(f\"\\nmy validation predictions  : {valid_pred}\")\n",
    "print(f\"true validation predictions: {y_valid}\")\n",
    "\n",
    "print()\n",
    "print(f\"my training predictions  : {train_pred}\")\n",
    "print(f\"true training predictions: {y_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-leonard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
