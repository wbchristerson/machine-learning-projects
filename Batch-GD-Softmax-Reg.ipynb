{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gothic-clothing",
   "metadata": {},
   "source": [
    "### An Implementation Of Batch Gradient Descent With $L_2$ Regularization And Early Stopping For Softmax Regression Without Using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eight-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2042)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expensive-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_test_split(X, y, test_ratio = 0.2, validation_ratio = 0.2):\n",
    "    total_size = len(X)\n",
    "    test_size = int(total_size * test_ratio)\n",
    "    valid_size = int(total_size * validation_ratio)\n",
    "    train_size = total_size - test_size - valid_size\n",
    "    \n",
    "    permutation_indices = np.random.permutation(total_size)\n",
    "    \n",
    "    X_train = X[permutation_indices[:train_size]]\n",
    "    y_train = y[permutation_indices[:train_size]]\n",
    "    X_valid = X[permutation_indices[train_size:-test_size]]\n",
    "    y_valid = y[permutation_indices[train_size:-test_size]]\n",
    "    X_test = X[permutation_indices[-test_size:]]\n",
    "    y_test = y[permutation_indices[-test_size:]]\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(y_output, possible_outputs):\n",
    "    m = len(y_output)\n",
    "    encoding = np.zeros((m, possible_outputs))\n",
    "    encoding[np.arange(m), y_output] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuing-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2,3)]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = my_test_split(X_with_bias, y)\n",
    "\n",
    "y_train_one_hot = encode_output(y_train, len(np.unique(y)))\n",
    "y_valid_one_hot = encode_output(y_valid, len(np.unique(y)))\n",
    "y_test_one_hot = encode_output(y_test, len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exps_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exps_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-january",
   "metadata": {},
   "source": [
    "$J(\\mathbf{\\Theta}) =\n",
    "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "\n",
    "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corresponding-carrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 2.1041298095182546\n",
      "iteration: 500, loss: 0.7975963791730118\n",
      "iteration: 1000, loss: 0.6623252726598646\n",
      "iteration: 1500, loss: 0.5815322702225296\n",
      "iteration: 2000, loss: 0.5285580975272502\n",
      "iteration: 2500, loss: 0.4907949263916581\n",
      "iteration: 3000, loss: 0.46208653508183384\n",
      "iteration: 3500, loss: 0.4391923433913653\n",
      "iteration: 4000, loss: 0.4202717004630981\n",
      "iteration: 4500, loss: 0.40420680212026194\n",
      "iteration: 5000, loss: 0.39027932970205237\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y))\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "iterations = 5001\n",
    "eta = 0.01\n",
    "\n",
    "theta = np.random.rand(n_inputs, n_outputs)\n",
    "for i in range(iterations):\n",
    "    logits = X_train.dot(theta)\n",
    "    y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(np.log(y_proba + epsilon) * y_train_one_hot, axis = 1))\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration: {i}, loss: {loss}\")\n",
    "    \n",
    "    error = y_proba - y_train_one_hot\n",
    "\n",
    "    gradients = 1/m * (X_train.T.dot(error))\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "successful-cannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.52684124, -0.18458978, -2.48941342],\n",
       "       [ 0.09459761,  1.08547944,  0.92677895],\n",
       "       [-1.37121463, -0.01431783,  1.96615452]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "monthly-elephant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-mumbai",
   "metadata": {},
   "source": [
    "### With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "desirable-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 4.7895118170337865\n",
      "iteration: 500, loss: 4.354613634138111\n",
      "iteration: 1000, loss: 4.303759867080397\n",
      "iteration: 1500, loss: 4.281378596617913\n",
      "iteration: 2000, loss: 4.269049337541218\n",
      "iteration: 2500, loss: 4.261617688966042\n",
      "iteration: 3000, loss: 4.256931573007697\n",
      "iteration: 3500, loss: 4.2539000488526195\n",
      "iteration: 4000, loss: 4.2519078975774125\n",
      "iteration: 4500, loss: 4.250585567880129\n",
      "iteration: 5000, loss: 4.24970206452791\n"
     ]
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y))\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "iterations = 5001\n",
    "eta = 0.1\n",
    "alpha = 0.1\n",
    "\n",
    "r_theta = np.random.rand(n_inputs, n_outputs)\n",
    "for i in range(iterations):\n",
    "    logits = X_train.dot(r_theta)\n",
    "    y_proba = softmax(logits)\n",
    "    entropy_loss = -np.mean(np.sum(np.log(y_proba + epsilon) * y_train_one_hot, axis = 1))\n",
    "    l2_loss = 0.5 * np.sum(np.square(theta[1:]))\n",
    "    loss = entropy_loss + l2_loss\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration: {i}, loss: {loss}\")\n",
    "    \n",
    "    error = y_proba - y_train_one_hot\n",
    "    gradients = 1/m * (X_train.T.dot(error)) + np.r_[np.zeros([1, n_outputs]), alpha * r_theta[1:]]\n",
    "    r_theta = r_theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bigger-governor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(r_theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-mortality",
   "metadata": {},
   "source": [
    "### With Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-private",
   "metadata": {},
   "source": [
    "$J(\\mathbf{\\Theta}) =\n",
    "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "\n",
    "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "raising-entity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss: 1.0824958946408876\n",
      "iteration 500, loss: 0.5283056103299459\n",
      "iteration 1000, loss: 0.502075721417513\n",
      "iteration 1500, loss: 0.4941190510171293\n",
      "iteration 2000, loss: 0.4910668305306171\n",
      "iteration 2500, loss: 0.48980162574234537\n",
      "iteration 3000, loss: 0.489256432825036\n",
      "iteration 3500, loss: 0.4890161892988417\n",
      "iteration 4000, loss: 0.48890885997582456\n",
      "iteration 4500, loss: 0.48886048947306704\n",
      "iteration 5000, loss: 0.48883856594338904\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 5001\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y))\n",
    "e_theta = np.random.rand(n_inputs, n_outputs)\n",
    "m = X_train.shape[0]\n",
    "eta = 0.1\n",
    "alpha = 0.1\n",
    "best_loss = np.infty\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    logits = X_train.dot(e_theta)\n",
    "    proba = softmax(logits)\n",
    "    \n",
    "    label_proba = np.sum(proba * y_train_one_hot, axis=1)\n",
    "    adjusted_label_proba = label_proba + epsilon\n",
    "    log_label_proba = np.log(adjusted_label_proba)\n",
    "    entropy_loss = -np.mean(log_label_proba)\n",
    "    l2_loss = np.sum(np.square(e_theta[1:])) / 2\n",
    "    loss = entropy_loss + alpha * l2_loss\n",
    "    # loss = np.sum(-np.log(np.sum(proba * y_train_one_hot, axis=1) + epsilon))\n",
    "    \n",
    "    gradient_matrix = X_train.T.dot(proba - y_train_one_hot) / m + alpha * np.r_[np.zeros([1, n_outputs]), e_theta[1:]]\n",
    "    e_theta -= eta * gradient_matrix\n",
    "    logits = X_train.dot(e_theta)\n",
    "    proba = softmax(logits)\n",
    "    label_proba = np.sum(proba * y_train_one_hot, axis=1)\n",
    "    adjusted_label_proba = label_proba + epsilon\n",
    "    log_label_proba = np.log(adjusted_label_proba)\n",
    "    entropy_loss = -np.mean(log_label_proba)\n",
    "    l2_loss = np.sum(np.square(e_theta[1:])) / 2\n",
    "    loss = entropy_loss + alpha * l2_loss\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration {i}, loss: {loss}\")\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(i-1, best_loss, \"best loss\")\n",
    "        print(i, loss, \"early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "pressing-aquatic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(e_theta)\n",
    "proba = softmax(logits)\n",
    "y_valid_predict = np.argmax(proba, axis = 1)\n",
    "\n",
    "accuracy_score = np.mean(y_valid == y_valid_predict)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "insured-medicare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.01603206],\n",
       "       [0.03206413],\n",
       "       [0.04809619],\n",
       "       [0.06412826],\n",
       "       [0.08016032],\n",
       "       [0.09619238],\n",
       "       [0.11222445],\n",
       "       [0.12825651],\n",
       "       [0.14428858],\n",
       "       [0.16032064],\n",
       "       [0.17635271],\n",
       "       [0.19238477],\n",
       "       [0.20841683],\n",
       "       [0.2244489 ],\n",
       "       [0.24048096],\n",
       "       [0.25651303],\n",
       "       [0.27254509],\n",
       "       [0.28857715],\n",
       "       [0.30460922],\n",
       "       [0.32064128],\n",
       "       [0.33667335],\n",
       "       [0.35270541],\n",
       "       [0.36873747],\n",
       "       [0.38476954],\n",
       "       [0.4008016 ],\n",
       "       [0.41683367],\n",
       "       [0.43286573],\n",
       "       [0.4488978 ],\n",
       "       [0.46492986],\n",
       "       [0.48096192],\n",
       "       [0.49699399],\n",
       "       [0.51302605],\n",
       "       [0.52905812],\n",
       "       [0.54509018],\n",
       "       [0.56112224],\n",
       "       [0.57715431],\n",
       "       [0.59318637],\n",
       "       [0.60921844],\n",
       "       [0.6252505 ],\n",
       "       [0.64128257],\n",
       "       [0.65731463],\n",
       "       [0.67334669],\n",
       "       [0.68937876],\n",
       "       [0.70541082],\n",
       "       [0.72144289],\n",
       "       [0.73747495],\n",
       "       [0.75350701],\n",
       "       [0.76953908],\n",
       "       [0.78557114],\n",
       "       [0.80160321],\n",
       "       [0.81763527],\n",
       "       [0.83366733],\n",
       "       [0.8496994 ],\n",
       "       [0.86573146],\n",
       "       [0.88176353],\n",
       "       [0.89779559],\n",
       "       [0.91382766],\n",
       "       [0.92985972],\n",
       "       [0.94589178],\n",
       "       [0.96192385],\n",
       "       [0.97795591],\n",
       "       [0.99398798],\n",
       "       [1.01002004],\n",
       "       [1.0260521 ],\n",
       "       [1.04208417],\n",
       "       [1.05811623],\n",
       "       [1.0741483 ],\n",
       "       [1.09018036],\n",
       "       [1.10621242],\n",
       "       [1.12224449],\n",
       "       [1.13827655],\n",
       "       [1.15430862],\n",
       "       [1.17034068],\n",
       "       [1.18637275],\n",
       "       [1.20240481],\n",
       "       [1.21843687],\n",
       "       [1.23446894],\n",
       "       [1.250501  ],\n",
       "       [1.26653307],\n",
       "       [1.28256513],\n",
       "       [1.29859719],\n",
       "       [1.31462926],\n",
       "       [1.33066132],\n",
       "       [1.34669339],\n",
       "       [1.36272545],\n",
       "       [1.37875752],\n",
       "       [1.39478958],\n",
       "       [1.41082164],\n",
       "       [1.42685371],\n",
       "       [1.44288577],\n",
       "       [1.45891784],\n",
       "       [1.4749499 ],\n",
       "       [1.49098196],\n",
       "       [1.50701403],\n",
       "       [1.52304609],\n",
       "       [1.53907816],\n",
       "       [1.55511022],\n",
       "       [1.57114228],\n",
       "       [1.58717435],\n",
       "       [1.60320641],\n",
       "       [1.61923848],\n",
       "       [1.63527054],\n",
       "       [1.65130261],\n",
       "       [1.66733467],\n",
       "       [1.68336673],\n",
       "       [1.6993988 ],\n",
       "       [1.71543086],\n",
       "       [1.73146293],\n",
       "       [1.74749499],\n",
       "       [1.76352705],\n",
       "       [1.77955912],\n",
       "       [1.79559118],\n",
       "       [1.81162325],\n",
       "       [1.82765531],\n",
       "       [1.84368737],\n",
       "       [1.85971944],\n",
       "       [1.8757515 ],\n",
       "       [1.89178357],\n",
       "       [1.90781563],\n",
       "       [1.9238477 ],\n",
       "       [1.93987976],\n",
       "       [1.95591182],\n",
       "       [1.97194389],\n",
       "       [1.98797595],\n",
       "       [2.00400802],\n",
       "       [2.02004008],\n",
       "       [2.03607214],\n",
       "       [2.05210421],\n",
       "       [2.06813627],\n",
       "       [2.08416834],\n",
       "       [2.1002004 ],\n",
       "       [2.11623246],\n",
       "       [2.13226453],\n",
       "       [2.14829659],\n",
       "       [2.16432866],\n",
       "       [2.18036072],\n",
       "       [2.19639279],\n",
       "       [2.21242485],\n",
       "       [2.22845691],\n",
       "       [2.24448898],\n",
       "       [2.26052104],\n",
       "       [2.27655311],\n",
       "       [2.29258517],\n",
       "       [2.30861723],\n",
       "       [2.3246493 ],\n",
       "       [2.34068136],\n",
       "       [2.35671343],\n",
       "       [2.37274549],\n",
       "       [2.38877756],\n",
       "       [2.40480962],\n",
       "       [2.42084168],\n",
       "       [2.43687375],\n",
       "       [2.45290581],\n",
       "       [2.46893788],\n",
       "       [2.48496994],\n",
       "       [2.501002  ],\n",
       "       [2.51703407],\n",
       "       [2.53306613],\n",
       "       [2.5490982 ],\n",
       "       [2.56513026],\n",
       "       [2.58116232],\n",
       "       [2.59719439],\n",
       "       [2.61322645],\n",
       "       [2.62925852],\n",
       "       [2.64529058],\n",
       "       [2.66132265],\n",
       "       [2.67735471],\n",
       "       [2.69338677],\n",
       "       [2.70941884],\n",
       "       [2.7254509 ],\n",
       "       [2.74148297],\n",
       "       [2.75751503],\n",
       "       [2.77354709],\n",
       "       [2.78957916],\n",
       "       [2.80561122],\n",
       "       [2.82164329],\n",
       "       [2.83767535],\n",
       "       [2.85370741],\n",
       "       [2.86973948],\n",
       "       [2.88577154],\n",
       "       [2.90180361],\n",
       "       [2.91783567],\n",
       "       [2.93386774],\n",
       "       [2.9498998 ],\n",
       "       [2.96593186],\n",
       "       [2.98196393],\n",
       "       [2.99799599],\n",
       "       [3.01402806],\n",
       "       [3.03006012],\n",
       "       [3.04609218],\n",
       "       [3.06212425],\n",
       "       [3.07815631],\n",
       "       [3.09418838],\n",
       "       [3.11022044],\n",
       "       [3.12625251],\n",
       "       [3.14228457],\n",
       "       [3.15831663],\n",
       "       [3.1743487 ],\n",
       "       [3.19038076],\n",
       "       [3.20641283],\n",
       "       [3.22244489],\n",
       "       [3.23847695],\n",
       "       [3.25450902],\n",
       "       [3.27054108],\n",
       "       [3.28657315],\n",
       "       [3.30260521],\n",
       "       [3.31863727],\n",
       "       [3.33466934],\n",
       "       [3.3507014 ],\n",
       "       [3.36673347],\n",
       "       [3.38276553],\n",
       "       [3.3987976 ],\n",
       "       [3.41482966],\n",
       "       [3.43086172],\n",
       "       [3.44689379],\n",
       "       [3.46292585],\n",
       "       [3.47895792],\n",
       "       [3.49498998],\n",
       "       [3.51102204],\n",
       "       [3.52705411],\n",
       "       [3.54308617],\n",
       "       [3.55911824],\n",
       "       [3.5751503 ],\n",
       "       [3.59118236],\n",
       "       [3.60721443],\n",
       "       [3.62324649],\n",
       "       [3.63927856],\n",
       "       [3.65531062],\n",
       "       [3.67134269],\n",
       "       [3.68737475],\n",
       "       [3.70340681],\n",
       "       [3.71943888],\n",
       "       [3.73547094],\n",
       "       [3.75150301],\n",
       "       [3.76753507],\n",
       "       [3.78356713],\n",
       "       [3.7995992 ],\n",
       "       [3.81563126],\n",
       "       [3.83166333],\n",
       "       [3.84769539],\n",
       "       [3.86372745],\n",
       "       [3.87975952],\n",
       "       [3.89579158],\n",
       "       [3.91182365],\n",
       "       [3.92785571],\n",
       "       [3.94388778],\n",
       "       [3.95991984],\n",
       "       [3.9759519 ],\n",
       "       [3.99198397],\n",
       "       [4.00801603],\n",
       "       [4.0240481 ],\n",
       "       [4.04008016],\n",
       "       [4.05611222],\n",
       "       [4.07214429],\n",
       "       [4.08817635],\n",
       "       [4.10420842],\n",
       "       [4.12024048],\n",
       "       [4.13627255],\n",
       "       [4.15230461],\n",
       "       [4.16833667],\n",
       "       [4.18436874],\n",
       "       [4.2004008 ],\n",
       "       [4.21643287],\n",
       "       [4.23246493],\n",
       "       [4.24849699],\n",
       "       [4.26452906],\n",
       "       [4.28056112],\n",
       "       [4.29659319],\n",
       "       [4.31262525],\n",
       "       [4.32865731],\n",
       "       [4.34468938],\n",
       "       [4.36072144],\n",
       "       [4.37675351],\n",
       "       [4.39278557],\n",
       "       [4.40881764],\n",
       "       [4.4248497 ],\n",
       "       [4.44088176],\n",
       "       [4.45691383],\n",
       "       [4.47294589],\n",
       "       [4.48897796],\n",
       "       [4.50501002],\n",
       "       [4.52104208],\n",
       "       [4.53707415],\n",
       "       [4.55310621],\n",
       "       [4.56913828],\n",
       "       [4.58517034],\n",
       "       [4.6012024 ],\n",
       "       [4.61723447],\n",
       "       [4.63326653],\n",
       "       [4.6492986 ],\n",
       "       [4.66533066],\n",
       "       [4.68136273],\n",
       "       [4.69739479],\n",
       "       [4.71342685],\n",
       "       [4.72945892],\n",
       "       [4.74549098],\n",
       "       [4.76152305],\n",
       "       [4.77755511],\n",
       "       [4.79358717],\n",
       "       [4.80961924],\n",
       "       [4.8256513 ],\n",
       "       [4.84168337],\n",
       "       [4.85771543],\n",
       "       [4.87374749],\n",
       "       [4.88977956],\n",
       "       [4.90581162],\n",
       "       [4.92184369],\n",
       "       [4.93787575],\n",
       "       [4.95390782],\n",
       "       [4.96993988],\n",
       "       [4.98597194],\n",
       "       [5.00200401],\n",
       "       [5.01803607],\n",
       "       [5.03406814],\n",
       "       [5.0501002 ],\n",
       "       [5.06613226],\n",
       "       [5.08216433],\n",
       "       [5.09819639],\n",
       "       [5.11422846],\n",
       "       [5.13026052],\n",
       "       [5.14629259],\n",
       "       [5.16232465],\n",
       "       [5.17835671],\n",
       "       [5.19438878],\n",
       "       [5.21042084],\n",
       "       [5.22645291],\n",
       "       [5.24248497],\n",
       "       [5.25851703],\n",
       "       [5.2745491 ],\n",
       "       [5.29058116],\n",
       "       [5.30661323],\n",
       "       [5.32264529],\n",
       "       [5.33867735],\n",
       "       [5.35470942],\n",
       "       [5.37074148],\n",
       "       [5.38677355],\n",
       "       [5.40280561],\n",
       "       [5.41883768],\n",
       "       [5.43486974],\n",
       "       [5.4509018 ],\n",
       "       [5.46693387],\n",
       "       [5.48296593],\n",
       "       [5.498998  ],\n",
       "       [5.51503006],\n",
       "       [5.53106212],\n",
       "       [5.54709419],\n",
       "       [5.56312625],\n",
       "       [5.57915832],\n",
       "       [5.59519038],\n",
       "       [5.61122244],\n",
       "       [5.62725451],\n",
       "       [5.64328657],\n",
       "       [5.65931864],\n",
       "       [5.6753507 ],\n",
       "       [5.69138277],\n",
       "       [5.70741483],\n",
       "       [5.72344689],\n",
       "       [5.73947896],\n",
       "       [5.75551102],\n",
       "       [5.77154309],\n",
       "       [5.78757515],\n",
       "       [5.80360721],\n",
       "       [5.81963928],\n",
       "       [5.83567134],\n",
       "       [5.85170341],\n",
       "       [5.86773547],\n",
       "       [5.88376754],\n",
       "       [5.8997996 ],\n",
       "       [5.91583166],\n",
       "       [5.93186373],\n",
       "       [5.94789579],\n",
       "       [5.96392786],\n",
       "       [5.97995992],\n",
       "       [5.99599198],\n",
       "       [6.01202405],\n",
       "       [6.02805611],\n",
       "       [6.04408818],\n",
       "       [6.06012024],\n",
       "       [6.0761523 ],\n",
       "       [6.09218437],\n",
       "       [6.10821643],\n",
       "       [6.1242485 ],\n",
       "       [6.14028056],\n",
       "       [6.15631263],\n",
       "       [6.17234469],\n",
       "       [6.18837675],\n",
       "       [6.20440882],\n",
       "       [6.22044088],\n",
       "       [6.23647295],\n",
       "       [6.25250501],\n",
       "       [6.26853707],\n",
       "       [6.28456914],\n",
       "       [6.3006012 ],\n",
       "       [6.31663327],\n",
       "       [6.33266533],\n",
       "       [6.34869739],\n",
       "       [6.36472946],\n",
       "       [6.38076152],\n",
       "       [6.39679359],\n",
       "       [6.41282565],\n",
       "       [6.42885772],\n",
       "       [6.44488978],\n",
       "       [6.46092184],\n",
       "       [6.47695391],\n",
       "       [6.49298597],\n",
       "       [6.50901804],\n",
       "       [6.5250501 ],\n",
       "       [6.54108216],\n",
       "       [6.55711423],\n",
       "       [6.57314629],\n",
       "       [6.58917836],\n",
       "       [6.60521042],\n",
       "       [6.62124248],\n",
       "       [6.63727455],\n",
       "       [6.65330661],\n",
       "       [6.66933868],\n",
       "       [6.68537074],\n",
       "       [6.70140281],\n",
       "       [6.71743487],\n",
       "       [6.73346693],\n",
       "       [6.749499  ],\n",
       "       [6.76553106],\n",
       "       [6.78156313],\n",
       "       [6.79759519],\n",
       "       [6.81362725],\n",
       "       [6.82965932],\n",
       "       [6.84569138],\n",
       "       [6.86172345],\n",
       "       [6.87775551],\n",
       "       [6.89378758],\n",
       "       [6.90981964],\n",
       "       [6.9258517 ],\n",
       "       [6.94188377],\n",
       "       [6.95791583],\n",
       "       [6.9739479 ],\n",
       "       [6.98997996],\n",
       "       [7.00601202],\n",
       "       [7.02204409],\n",
       "       [7.03807615],\n",
       "       [7.05410822],\n",
       "       [7.07014028],\n",
       "       [7.08617234],\n",
       "       [7.10220441],\n",
       "       [7.11823647],\n",
       "       [7.13426854],\n",
       "       [7.1503006 ],\n",
       "       [7.16633267],\n",
       "       [7.18236473],\n",
       "       [7.19839679],\n",
       "       [7.21442886],\n",
       "       [7.23046092],\n",
       "       [7.24649299],\n",
       "       [7.26252505],\n",
       "       [7.27855711],\n",
       "       [7.29458918],\n",
       "       [7.31062124],\n",
       "       [7.32665331],\n",
       "       [7.34268537],\n",
       "       [7.35871743],\n",
       "       [7.3747495 ],\n",
       "       [7.39078156],\n",
       "       [7.40681363],\n",
       "       [7.42284569],\n",
       "       [7.43887776],\n",
       "       [7.45490982],\n",
       "       [7.47094188],\n",
       "       [7.48697395],\n",
       "       [7.50300601],\n",
       "       [7.51903808],\n",
       "       [7.53507014],\n",
       "       [7.5511022 ],\n",
       "       [7.56713427],\n",
       "       [7.58316633],\n",
       "       [7.5991984 ],\n",
       "       [7.61523046],\n",
       "       [7.63126253],\n",
       "       [7.64729459],\n",
       "       [7.66332665],\n",
       "       [7.67935872],\n",
       "       [7.69539078],\n",
       "       [7.71142285],\n",
       "       [7.72745491],\n",
       "       [7.74348697],\n",
       "       [7.75951904],\n",
       "       [7.7755511 ],\n",
       "       [7.79158317],\n",
       "       [7.80761523],\n",
       "       [7.82364729],\n",
       "       [7.83967936],\n",
       "       [7.85571142],\n",
       "       [7.87174349],\n",
       "       [7.88777555],\n",
       "       [7.90380762],\n",
       "       [7.91983968],\n",
       "       [7.93587174],\n",
       "       [7.95190381],\n",
       "       [7.96793587],\n",
       "       [7.98396794],\n",
       "       [8.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 8, 500).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-porter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
